# -*- coding: utf-8 -*-
"""Modelbenih3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pxUnzmeDQ3UFKHPJlnnKguDDhyFN8QQ2

# Import Data dan Library
"""

# Mempersiapkan Library Tensorflow & Sklearn
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam

import sklearn
from sklearn import datasets
from sklearn.model_selection import train_test_split

import os
print(tf.__version__)

pip install split_folders

# Download dataset benih dari web 
!wget --no-check-certificate \
  https://indigief.com/datasetbenihc4.zip \
  -O /datasetbenihc4.zip

# Melakukan ekstraksi pada file zip
import zipfile,os
local_zip = '/datasetbenihc4.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/')
zip_ref.close()

# Melihat struktur data yang terunduh
os.listdir('/datasetbenihc4')

# Menampilkan jumlah data masing-masing folder
print('total Benih Unggul images :', len(os.listdir('/datasetbenihc4/unggul')))
print('total Benih Tidak Unggul images :', len(os.listdir('/datasetbenihc4/tidakunggul')))

"""# Data Prepocessing"""

!pip3 install split-folders tqdm

# Melakukan pembagian data training dan data validasi
import splitfolders
base_dir = "/datasetbenihc4"
splitfolders.ratio(base_dir, output = "/datasetbenihc4", seed = 1000, ratio = (.8, .2))

train_dir = os.path.join("/datasetbenihc4", "train")
validation_dir = os.path.join("/datasetbenihc4", "val")

# Membuat direktori kelas unggul dan tidak unggul
unggul_dir = os.path.join (base_dir, 'unggul')
tidakunggul_dir = os.path.join (base_dir, 'tidakunggul')

train_unggul_fnames = os.listdir(unggul_dir)
print(train_unggul_fnames[:5])

train_tidakunggul_fnames = os.listdir(tidakunggul_dir)
train_tidakunggul_fnames.sort()
print(train_tidakunggul_fnames[:5])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Parameters for our graph; we'll output images in a 4x4 configuration
nrows = 1
ncols = 3

# Index for iterating over images
pic_index = 0

# Set up matplotlib fig, and size it to fit 3x3 pics
fig = plt.gcf()
fig.set_size_inches(nrows * 8, ncols * 8)
pic_index += 1
next_unggul_pix = [os.path.join(unggul_dir, fname) 
                for fname in train_unggul_fnames[pic_index-1:pic_index]]

next_tidakunggul_pix = [os.path.join(tidakunggul_dir, fname) 
                for fname in train_tidakunggul_fnames[pic_index-1:pic_index]]

for i, img_path in enumerate(next_unggul_pix+next_tidakunggul_pix):
  # Set up subplot; subplot indices start at 1
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off') # Don't show axes (or gridlines)

  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

#memecah direktori benih unggul menjadi data train 80% dan validasi data 20%
train_unggul_dir, val_unggul_dir = train_test_split (os.listdir(unggul_dir), train_size = 0.8, test_size = 0.2)

#memecah direktori benih tidak unggul menjadi data train 80% dan  validasi data 20%
train_tidakunggul_dir, val_tidakunggul_dir = train_test_split (os.listdir(tidakunggul_dir), train_size = 0.8, test_size = 0.2)

train_unggul = os.path.join (train_dir, 'unggul')
train_tidakunggul = os.path.join (train_dir, 'tidakunggul')

val_unggul = os.path.join (validation_dir, 'unggul')
val_tidakunggul = os.path.join (validation_dir, 'tidakunggul')

train_datagen = ImageDataGenerator(
                    rescale = 1./255,
                    rotation_range = 20,
                    horizontal_flip = True,
                    shear_range = 0.2,
                    fill_mode ='nearest')

test_datagen = ImageDataGenerator(
                    rescale = 1./255,
                    rotation_range = 20,
                    horizontal_flip = True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')

train_generator = train_datagen.flow_from_directory(
                    train_dir,
                    target_size = (150, 150),  # mengubah resolusi seluruh gambar menjadi 150x150 piksel
                    class_mode = 'categorical')

validation_generator = test_datagen.flow_from_directory(
                    validation_dir,
                    target_size = (150,150),  # mengubah resolusi seluruh gambar menjadi 150x150 piksel
                    class_mode = 'categorical')

"""# Pemodelan"""

# Arsitektur CNN dalam Perancangan Model

model = tf.keras.models.Sequential()

# First convolution.
model.add(tf.keras.layers.Convolution2D(
    input_shape=(150, 150, 3),
    filters=64,
    kernel_size=3,
    activation=tf.keras.activations.relu
))
model.add(tf.keras.layers.MaxPooling2D(
    pool_size=(2, 2),
    strides=(2, 2)
))

# Second convolution.
model.add(tf.keras.layers.Convolution2D(
    filters=64,
    kernel_size=3,
    activation=tf.keras.activations.relu
))
model.add(tf.keras.layers.MaxPooling2D(
    pool_size=(2, 2),
    strides=(2, 2)
))

# Third convolution.
model.add(tf.keras.layers.Convolution2D(
    filters=128,
    kernel_size=3,
    activation=tf.keras.activations.relu
))
model.add(tf.keras.layers.MaxPooling2D(
    pool_size=(2, 2),
    strides=(2, 2)
))

# Fourth convolution.
model.add(tf.keras.layers.Convolution2D(
    filters=128,
    kernel_size=3,
    activation=tf.keras.activations.relu
))
model.add(tf.keras.layers.MaxPooling2D(
    pool_size=(2, 2),
    strides=(2, 2)
))

# Flatten the results to feed into dense layers.
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dropout(0.5))

# 512 neuron dense layer.
model.add(tf.keras.layers.Dense(
    units=512,
    activation=tf.keras.activations.relu
))

# Output layer.
model.add(tf.keras.layers.Dense(
    units=2,
    activation=tf.keras.activations.softmax
))

model.summary()

#tf.keras.utils.plot_model(
    #model,
    #show_shapes=True,
    #show_layer_names=True,)

# Penggunaan Optimasi Model

adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(
  optimizer=adam_optimizer,
  loss='categorical_crossentropy',
  metrics=['accuracy']
)

!rm -rf tmp/checkpoints
!rm -rf logs

"""# Train Dataset"""

# Pelatihan data terhadap model yang dirancang
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(
      train_generator,
      batch_size=64,
      steps_per_epoch=18,
      epochs=50,
      callbacks=[callbacks],
      validation_data=validation_generator,
      validation_steps=5,
      verbose=2,)

# Evaluasi performa model dalam Pengujian data
loss, accuracy = model.evaluate(validation_generator)

print("\nModel's Evaluation Metrics: ")
print("---------------------------")
print("Accuracy: {} \nLoss: {}".format(accuracy, loss))

# Tampilan Grafik Nilai Akurasi data training dan validasi

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Tampilan Grafik Nilai Loss data training dan validasi

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Pengujian prediksi data baru terhadap hasil model

import numpy as np
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mping
import matplotlib.pyplot as plt
import matplotlib.image as mping
# %matplotlib inline

uploaded = files.upload()

for fn in uploaded.keys():
  
  #predicting images
  path = fn
  img = image.load_img(path, target_size= (150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=100)
  
  #convert numpy array into string
  result = np.array_str(classes) 
  print(result)
  
  print(fn)
  if classes[0][1]==1:
    print('Result : Benih Unggul')
  elif classes[0][0]==1:
    print('Result : Benih Tidak Unggul')

model.save('klasifikasi_benih_model3.h5')

! pip install tensorflowjs

! tensorflowjs_converter --input_format keras klasifikasi_benih_model3.h5 ./tfjs/